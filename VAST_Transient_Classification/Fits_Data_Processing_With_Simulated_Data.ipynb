{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce32705e-0c85-4de0-9ac8-896d1ce6c848",
   "metadata": {},
   "source": [
    "# Import The Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c426d73-3036-4717-86bd-42f7cf5abb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "from astropy.io import fits\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import astropy.units as u\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d50e32-48c4-4d02-997d-c3cd4c57aa52",
   "metadata": {},
   "source": [
    "# Obtain and Process The Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ed20b0-54d7-44dd-9e13-32a6af0bc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data = pd.read_csv(\"All_Transient_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0fe7d8-349a-4fd8-9b33-1c68c667324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data = t_data[[\"sbid\",\"beam\",\"name\",\"PSR_Label\"]]\n",
    "# Remove all NaN values\n",
    "rel_data = rel_data.dropna(how='any',axis=0)\n",
    "rel_data[\"sbid\"] = rel_data[\"sbid\"].astype(str)\n",
    "rel_data[\"fits_path\"] = \"SB\"+rel_data[\"sbid\"]+\"_\"+rel_data[\"beam\"]+\"_slices_\"+rel_data[\"name\"]+\".fits\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1879de4-371d-47be-a7ff-4f31b60619fd",
   "metadata": {},
   "source": [
    "## Obtain Relevant Images Slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c9297-e561-45df-ac9c-13710cb0a15f",
   "metadata": {},
   "source": [
    "### Obtain a Balanced Light Curve Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018cb2cc-1d83-4173-898e-3b9829a85dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_src_name(avail_fits, chosen_idx):\n",
    "    \"\"\"\n",
    "    Used to obtain the source name and other details of particular fits paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    avail_fits : list\n",
    "        List of strings of fits paths\n",
    "\n",
    "    chosen_idx : int\n",
    "        Index of particular fits path in avail_fits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    src_name : str\n",
    "        Source name of fits path.\n",
    "\n",
    "    src_details : list\n",
    "        List of strings containing SBID, beam ID and source name\n",
    "    \"\"\"\n",
    "    \n",
    "    src_details = avail_fits[chosen_idx].split(\"/\")[1].split(\"_\")\n",
    "    src_name = src_details[-1][:-5]\n",
    "    return src_name, src_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73a77fc-7a17-4589-95b5-1a18d88751ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_lc_df(lc_folder_name, src_name, src_details, chosen_idx, real_cand):\n",
    "    \"\"\"\n",
    "    Used to obtain a dataframe for the light curve information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lc_folder_name : float\n",
    "        Path to light curve folder\n",
    "\n",
    "    src_name : float\n",
    "        Source name\n",
    "\n",
    "    src_details : list\n",
    "        Source details\n",
    "\n",
    "    chosen_idx : int\n",
    "        Index of label of interest in the labels\n",
    "\n",
    "    real_cand : list\n",
    "        List of strings of labels for the fits data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lc_df : pandas dataframe\n",
    "        A dataframe containing \"Time\", \"Time_Idx\", \"peak_flux\", \"local_rms\", \"Above_Threshold\" and \"Source_Info\" columns\n",
    "    \"\"\"\n",
    "    \n",
    "    lc_local_rms = pd.read_csv(f\"{lc_folder_name}/{src_details[0]}_{src_details[1]}_lightcurve_local_rms.csv\")\n",
    "    lc_local_rms = lc_local_rms.rename(columns = {src_name: \"local_rms\"})\n",
    "    lc_peak_flux = pd.read_csv(f\"{lc_folder_name}/{src_details[0]}_{src_details[1]}_lightcurve_peak_flux.csv\")\n",
    "    lc_peak_flux = lc_peak_flux.rename(columns = {src_name: \"peak_flux\"})\n",
    "    lc_df = pd.merge(lc_local_rms, lc_peak_flux, on=\"Time\")\n",
    "    # Remove unnecessary columns\n",
    "    lc_df = lc_df[[\"Time\", \"peak_flux\", \"local_rms\"]]\n",
    "    # Obtain indices of each time value so getting fits image is easier.\n",
    "    lc_df[\"Time_Idx\"] = lc_df.index\n",
    "\n",
    "    # Drop rows with null values\n",
    "    lc_df = lc_df.dropna()\n",
    "\n",
    "    # Remove rows where local_rms = 0\n",
    "    lc_df = lc_df[lc_df[\"local_rms\"] != 0].reset_index(drop=True) #### NOT SURE IF THIS IS ALLOWED\n",
    "\n",
    "    # Obtaining Labels\n",
    "    threshold = 5 \n",
    "    # (peak flux - mean peak flux) / local rms > threshold\n",
    "    \n",
    "    for row in lc_df:\n",
    "        if real_cand[chosen_idx] == 1:\n",
    "            # 1 is true positive and 0 is non-detection\n",
    "            lc_df[\"Above_Threshold\"] = np.where((np.abs(lc_df[\"peak_flux\"]-lc_df[\"peak_flux\"].mean())/ lc_df[\"local_rms\"] > threshold), 1, 0)\n",
    "        else:\n",
    "            # 2 is false positive and 0 is non-detection\n",
    "            lc_df[\"Above_Threshold\"] = np.where((np.abs(lc_df[\"peak_flux\"]-lc_df[\"peak_flux\"].mean())/ lc_df[\"local_rms\"] > threshold), 2, 0)\n",
    "    lc_df[\"Source_Info\"] = f\"{src_details[0]}_{src_details[1]}_{src_name}\"\n",
    "    return lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ff328d-8853-4545-9233-d8d7311250c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_balanced_lc_df(lc_df, random_state = 43, ratio_d_to_nd = 2):\n",
    "    \"\"\"\n",
    "    Used to obtain a balanced dataframe whereby the ratio of true positive\n",
    "    or false positive candidates to non detections are in a ratio_d_to_nd:1 ratio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lc_df : pandas dataframe\n",
    "        A dataframe with relevant light curve information.\n",
    "\n",
    "    random_state : int, default 43\n",
    "        The random state used for reproducibility.\n",
    "        \n",
    "    ratio_d_to_nd : int, default 2\n",
    "        The ratio of positives (either true or false) to non detections.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    balanced_lc_df_ls : list\n",
    "        A list containing light curve dataframes whereby the positives and non-detections \n",
    "        are balanced according to the given ratio.\n",
    "\n",
    "    positive_counts : int\n",
    "        Number of positives in the balanced light curve list\n",
    "    \"\"\"\n",
    "    \n",
    "    balanced_lc_df_ls = []\n",
    "    # To adjust the ratio between positives and non-detection, change num_cands\n",
    "    if sum(lc_df[\"Above_Threshold\"] != 0) <=  sum(lc_df[\"Above_Threshold\"] == 0):\n",
    "        num_cands = sum(lc_df[\"Above_Threshold\"] != 0)\n",
    "        nd_lc_data = lc_df[lc_df[\"Above_Threshold\"] == 0] # non detections\n",
    "        reduced_nd_lc_data = nd_lc_data.sample(n=round(num_cands/ratio_d_to_nd), random_state = random_state, axis = 0) # Want 2:1 ratio between detections and non-detections so divide num_cands by 2\n",
    "        balanced_lc_df_ls = [lc_df[lc_df[\"Above_Threshold\"] != 0], reduced_nd_lc_data]\n",
    "    else:\n",
    "        num_cands = sum(lc_df[\"Above_Threshold\"] == 0)\n",
    "        lc_data = lc_df[lc_df[\"Above_Threshold\"] != 0] # positives\n",
    "        reduced_lc_data = lc_data.sample(n=num_cands, random_state = random_state, axis = 0)\n",
    "        nd_lc_data = lc_df[lc_df[\"Above_Threshold\"] == 0] # non detections\n",
    "        reduced_nd_lc_data = nd_lc_data.sample(n=round(num_cands/ratio_d_to_nd), random_state = random_state, axis = 0)  # Want 2:1 ratio between detections and non-detections so divide num_cands by 2\n",
    "        balanced_lc_df_ls = [reduced_lc_data, reduced_nd_lc_data]\n",
    "        \n",
    "    if len(balanced_lc_df_ls) != 0:\n",
    "        positive_counts = len(balanced_lc_df_ls[0])\n",
    "    else:\n",
    "        positive_counts = 0\n",
    "    return balanced_lc_df_ls, positive_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8180141-f6f1-4bbf-88a6-852201580676",
   "metadata": {},
   "source": [
    "#### Finding Number of Occurrences of Each Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923a8a61-2a95-4516-b275-cb04dfe94323",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_folder_name = \"VAST 10s lightcurve\"\n",
    "fits_folder_name = \"VAST 10s fitscube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf50c22-8c9f-44dd-8410-5f44755f91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files in fits_folder and the actual data\n",
    "avail_fits = []\n",
    "# List of real candidates\n",
    "real_cand = []\n",
    "for file in os.listdir(fits_folder_name):\n",
    "    if file in rel_data[\"fits_path\"].unique():\n",
    "        avail_fits.append(fits_folder_name+\"/\"+file)\n",
    "        real_cand.append(rel_data[rel_data[\"fits_path\"]==file][\"PSR_Label\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411a9b61-f446-40e5-8f6f-8059e4877987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_positive_counts(avail_fits, label):\n",
    "    \"\"\"\n",
    "    Counts the total number of positives, either\n",
    "    true or false positives.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    avail_fits : list\n",
    "        List of strings of the available fits paths\n",
    "    \n",
    "    label : int\n",
    "        Either 1 for true positive or 0 for false positive\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_count_dict : dict\n",
    "        Dictionary which maps the indices of the fits paths in the avail_fits\n",
    "        to the count of positives for the associated light curve dataframe\n",
    "\n",
    "    total : int\n",
    "        Total number of positives\n",
    "    \"\"\"\n",
    "    #label = 1 for true positive and label = 0  for false positive\n",
    "    chosen_idx_ls = [i for i,val in enumerate(real_cand) if val==label]\n",
    "    idx_count_dict = {}\n",
    "    total = 0\n",
    "    for chosen_idx in chosen_idx_ls:\n",
    "        src_name, src_details = obtain_src_name(avail_fits, chosen_idx)\n",
    "        if os.path.isfile(f\"{lc_folder_name}/{src_details[0]}_{src_details[1]}_lightcurve_local_rms.csv\"):\n",
    "            lc_df = obtain_lc_df(lc_folder_name, src_name, src_details, chosen_idx, real_cand)\n",
    "            _ , positive_counts = obtain_balanced_lc_df(lc_df) # extended balanced_lc_df list\n",
    "            idx_count_dict[chosen_idx] = positive_counts\n",
    "            total += positive_counts\n",
    "        else:\n",
    "            continue\n",
    "    return idx_count_dict, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ffe9833-da86-4de7-84f3-cfbcc2ea8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_fp_count_dict, total_fp = obtain_positive_counts(avail_fits, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd871636-6dd2-4682-a062-464989aeb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tp_count_dict, total_tp = obtain_positive_counts(avail_fits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aced8a7-687f-4d3b-be60-6aa87641cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18895\n"
     ]
    }
   ],
   "source": [
    "print(total_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa4c105-c03b-4fe0-bad2-18f6afe2872a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611\n"
     ]
    }
   ],
   "source": [
    "print(total_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf53551-305c-46a4-a58e-ace39af1adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "tp_src_idx = list(idx_tp_count_dict.keys())\n",
    "print(len(tp_src_idx)) # number of true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63d1149-b31f-4ade-b930-1230b4bb468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2277\n"
     ]
    }
   ],
   "source": [
    "fp_src_idx = list(idx_fp_count_dict.keys())\n",
    "print(len(fp_src_idx)) # number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb53d3-93bb-4a66-83e9-b9a9ca074081",
   "metadata": {},
   "source": [
    "#### Concatenate the false positives and true positives into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73646b55-883a-4e9d-aa09-38f4c22d3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_balanced_df(src_idx):\n",
    "    \"\"\"\n",
    "    Stacks all the balanced dataframes obtained\n",
    "    from the light curves of all the relevant fits paths\n",
    "    into 1 dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    avail_fits : list\n",
    "        List of strings of the available fits paths\n",
    "    \n",
    "    src_idx : list\n",
    "        List of integers of indices for fits paths in avail_fits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    balanced_lc_df : pandas dataframe\n",
    "        A dataframe containing the data of all the other balanced dataframes\n",
    "        of the relevant light curves.\n",
    "    \"\"\"\n",
    "    \n",
    "    balanced_lc_df_ls = []\n",
    "\n",
    "    # Stack all the balanced light curves for every source with false/ true positives on top of each other\n",
    "    for idx in src_idx:\n",
    "        src_name, src_details = obtain_src_name(avail_fits, idx)\n",
    "        lc_df = obtain_lc_df(lc_folder_name, src_name, src_details, idx, real_cand)\n",
    "        new_balanced_lc_df, _ = obtain_balanced_lc_df(lc_df)\n",
    "        balanced_lc_df_ls.extend(new_balanced_lc_df)\n",
    "\n",
    "    balanced_lc_df = pd.concat(balanced_lc_df_ls).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94117eaf-2106-4a0b-ab37-baa7b47a417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_balanced_lc_df = stack_balanced_df(fp_src_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b38083bf-6244-4142-9896-e01b13cdbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28259, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_balanced_lc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac09489c-6a2c-422f-90cd-ff8780d95b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Above_Threshold\n",
       "2    18895\n",
       "0     9364\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_balanced_lc_df[\"Above_Threshold\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4353811-8026-4c7a-ae3a-849e2f9b505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_balanced_lc_df = stack_balanced_df(tp_src_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b576913-c34c-4928-86f0-b51cc53cdc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Above_Threshold\n",
       "1    1611\n",
       "0     793\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_balanced_lc_df[\"Above_Threshold\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce66f269-17b6-4493-a853-8bf81c929440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2404, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_balanced_lc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97005187-99fc-4918-b093-1ade6a633cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_balanced_df = pd.concat([fp_balanced_lc_df, tp_balanced_lc_df], ignore_index=True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d50ffa59-f3ac-46d4-a82c-ce4b26d03cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Above_Threshold\n",
       "2    18895\n",
       "0    10157\n",
       "1     1611\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_balanced_df[\"Above_Threshold\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c267c40-5bfe-491b-9a73-09f5206025b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30663"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_balanced_df[\"Above_Threshold\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ce980-b59d-4b3e-b451-2400ca4fdbdc",
   "metadata": {},
   "source": [
    "## Splitting The Data into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca8d4ab1-8a1d-4647-902e-7f0f2386092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_data(final_balanced_df, prop, det_src_type = 0, thresh = 5):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing data. The incoming data\n",
    "    has only true positives and non detections or only false positives\n",
    "    and non detections.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    final_balanced_df : pandas dataframe\n",
    "        A light curve dataframe with a balanced number of\n",
    "        positives and non detections.\n",
    "\n",
    "    prop : float\n",
    "        The proportion of the data to go into the training\n",
    "        data.\n",
    "\n",
    "    det_src_type : 0, default 0\n",
    "        If det_src_type is 0 then the detected sources are true positives,\n",
    "        if det_src_type is 1 then the detected sources are false positives\n",
    "\n",
    "    thresh : int, default 5\n",
    "        Will exit the while loop when the number of positives\n",
    "        in the training data is just within the calculated number of proportion of positives\n",
    "        in the training data plus this threshold value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_df : pandas dataframe\n",
    "        The light curve dataframe to be used for training\n",
    "\n",
    "    test_df : pandas dataframe\n",
    "        The light curve dataframe to be used for testing\n",
    "    \"\"\"\n",
    "    # det_src_type = 0, then the detected sources are true positives, and if they are 1 then the detected sources are false positives\n",
    "    train_count = round(prop * sum(final_balanced_df[\"Above_Threshold\"]==det_src_type+1)) # desired train_count\n",
    "    sources = list(final_balanced_df[final_balanced_df[\"Above_Threshold\"]==det_src_type+1][\"Source_Info\"].unique())\n",
    "    act_train_count = 0 # Actual train_count that we keep adding to\n",
    "    chsen_srcs = []\n",
    "    while act_train_count < train_count:\n",
    "        # Randomly choose sources and then remove the source from the array to ensure no duplicates.\n",
    "        sel_src = np.random.choice(sources)\n",
    "        new_count = sum(final_balanced_df[final_balanced_df[\"Source_Info\"] == sel_src][\"Above_Threshold\"] != 0) # Only consider the count of true/ false positives\n",
    "        if act_train_count + new_count > train_count + thresh:\n",
    "            continue\n",
    "        else:\n",
    "            act_train_count += new_count\n",
    "            chsen_srcs.append(sel_src)\n",
    "            sources.remove(sel_src)\n",
    "\n",
    "    non_chsen_srcs = list(set(sources)-set(chsen_srcs))\n",
    "    train_df = final_balanced_df[final_balanced_df[\"Source_Info\"].isin(chsen_srcs)]\n",
    "\n",
    "    # Put rest of data with \"Above_Threshold\" value == det_src_type+1 into test df.\n",
    "    test_df = final_balanced_df[final_balanced_df[\"Source_Info\"].isin(non_chsen_srcs)]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3196ddc5-7214-4180-914f-f1295b56a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train_df, fp_test_df = split_train_test_data(final_balanced_df, 0.7, det_src_type = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8ce2120-e9de-41a6-8d23-54c8992909e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train_df, fp_val_df = split_train_test_data(fp_train_df, 0.7, det_src_type = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6eed5c3b-cc18-4c37-a460-4ae7160ffa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_train_df, tp_test_df = split_train_test_data(final_balanced_df, 0.7, det_src_type = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ec6f407-125e-4cf4-8f3b-7d2bba61dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_train_df, tp_val_df = split_train_test_data(tp_train_df, 0.7, det_src_type = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d783e8-047a-493a-9f8d-7c665cce0d48",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5fdefa-0332-47da-95ec-c87ee50a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43\n",
    "train_df = pd.concat([tp_train_df, fp_train_df], ignore_index=True, axis = 0)\n",
    "# randomise the rows\n",
    "train_df = train_df.sample(frac=1, random_state = random_state, axis=0)\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d42ee587-f346-4208-81ea-48c1c1d4f9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>peak_flux</th>\n",
       "      <th>local_rms</th>\n",
       "      <th>Time_Idx</th>\n",
       "      <th>Above_Threshold</th>\n",
       "      <th>Source_Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-06-14T08:34:34.780029</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>SB62877_beam18_J082015.63-411435.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-06-07T15:57:52.708989</td>\n",
       "      <td>0.184731</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>SB62643_beam31_J175258.58-280637.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-04-22T19:17:59.240063</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>SB61530_beam17_J182530.54-093523.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2024-04-04T21:57:31.420030</td>\n",
       "      <td>0.142498</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>SB60803_beam26_J175258.73-280636.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2024-06-23T15:25:43.542141</td>\n",
       "      <td>0.178184</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>SB63235_beam31_J175258.58-280637.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14950</th>\n",
       "      <td>2024-04-04T21:39:06.605950</td>\n",
       "      <td>0.130343</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>SB60802_beam19_J175258.66-280637.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14960</th>\n",
       "      <td>2024-04-19T19:25:02.475647</td>\n",
       "      <td>0.544332</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>SB61302_beam11_J164449.31-455910.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>2024-04-16T19:34:55.691135</td>\n",
       "      <td>0.159635</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>SB61159_beam31_J175258.58-280637.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>2024-04-05T21:23:12.639356</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>SB60866_beam03_J185257.51-063559.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15008</th>\n",
       "      <td>2024-04-19T21:02:15.097726</td>\n",
       "      <td>0.170242</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SB61309_beam19_J175258.66-280637.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Time  peak_flux  local_rms  Time_Idx  \\\n",
       "8      2024-06-14T08:34:34.780029   0.001022   0.001555        16   \n",
       "36     2024-06-07T15:57:52.708989   0.184731   0.003788        24   \n",
       "37     2024-04-22T19:17:59.240063   0.041809   0.002569        34   \n",
       "107    2024-04-04T21:57:31.420030   0.142498   0.003661        38   \n",
       "111    2024-06-23T15:25:43.542141   0.178184   0.002973        11   \n",
       "...                           ...        ...        ...       ...   \n",
       "14950  2024-04-04T21:39:06.605950   0.130343   0.002121        17   \n",
       "14960  2024-04-19T19:25:02.475647   0.544332   0.002875        28   \n",
       "14974  2024-04-16T19:34:55.691135   0.159635   0.002889        29   \n",
       "14999  2024-04-05T21:23:12.639356   0.017459   0.001392        67   \n",
       "15008  2024-04-19T21:02:15.097726   0.170242   0.002109         5   \n",
       "\n",
       "       Above_Threshold                          Source_Info  \n",
       "8                    1  SB62877_beam18_J082015.63-411435.09  \n",
       "36                   1  SB62643_beam31_J175258.58-280637.95  \n",
       "37                   1  SB61530_beam17_J182530.54-093523.39  \n",
       "107                  1  SB60803_beam26_J175258.73-280636.72  \n",
       "111                  1  SB63235_beam31_J175258.58-280637.95  \n",
       "...                ...                                  ...  \n",
       "14950                1  SB60802_beam19_J175258.66-280637.15  \n",
       "14960                1  SB61302_beam11_J164449.31-455910.49  \n",
       "14974                1  SB61159_beam31_J175258.58-280637.95  \n",
       "14999                1  SB60866_beam03_J185257.51-063559.86  \n",
       "15008                1  SB61309_beam19_J175258.66-280637.15  \n",
       "\n",
       "[795 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"Above_Threshold\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2bf57-b194-4687-b611-f4bcb1fbd756",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a2c0321-83aa-404e-b62f-764efa08ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.concat([tp_val_df, fp_val_df], ignore_index=True, axis = 0)\n",
    "# randomise the rows\n",
    "val_df = val_df.sample(frac=1, random_state = random_state, axis=0)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32211d29-ea07-4d1a-99cd-1968e503700a",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edf638c2-d6c9-46ee-9648-6be52fd54d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([tp_test_df, fp_test_df], ignore_index=True, axis = 0)\n",
    "# randomise the rows\n",
    "test_df = test_df.sample(frac=1, random_state = random_state, axis=0)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85528ac8-2dd0-454b-a713-0133567e5f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9192, 6)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d8659-991d-40cb-9296-bc6217566448",
   "metadata": {},
   "source": [
    "#### Test Sources Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a316f0-c4cb-422a-8366-629a28b58aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df[\"Source_Info\"]).intersection(set(test_df[\"Source_Info\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e6f4872-53a5-4eb9-9c82-e99390633fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df[\"Source_Info\"]).intersection(set(val_df[\"Source_Info\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60992d8e-2c0b-45bd-883e-1c9ad3fe1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_df[\"Source_Info\"]).intersection(set(val_df[\"Source_Info\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33c96a-62ae-4817-9cca-bb79e0b354af",
   "metadata": {},
   "source": [
    "#### Obtain The Relevant Fits Files for Each Source and Preprocess The Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d433960b-a53a-4aba-bd53-099c05d26072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_slices_and_labels(df, fits_fol = \"VAST 10s fitscube\"):\n",
    "    \"\"\"\n",
    "    Obtain the images and labels that the classifier\n",
    "    will be trained on.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Light curve dataframe\n",
    "\n",
    "    fits_fol : str, default \"VAST 10s fitscube\"\n",
    "        Path to the fits cube folder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_labels : pandas series\n",
    "        Labels for the images\n",
    "\n",
    "    img_slice : numpy array\n",
    "        The images to classify\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the relevant image slices and their associated labels for a particular dataframe - test, train or val.\n",
    "    img_labels = df[\"Above_Threshold\"] # remove labels which do not correspond to actual image\n",
    "    keep_idx = []\n",
    "    img_slice = np.empty(shape=(len(img_labels),120,120,3))\n",
    "    for i, row in df.iterrows():\n",
    "        src_info = row.loc[\"Source_Info\"].split(\"_\")\n",
    "        src_info.insert(2, \"slices\")\n",
    "        fits_filename = f'{fits_fol}/{\"_\".join(src_info)}.fits'\n",
    "        hdu = fits.open(fits_filename)\n",
    "        data = hdu[0].data\n",
    "\n",
    "        new_img_slice = data[row.loc[\"Time_Idx\"]]\n",
    "\n",
    "        if new_img_slice.shape != (120,120):\n",
    "            continue\n",
    "        # Since this is a valid image let's keep this image\n",
    "        keep_idx.append(i)\n",
    "        \n",
    "        # Triplicate to obtain RGB from binary.\n",
    "        new_img_slice = np.repeat(new_img_slice[...,np.newaxis], 3, -1)\n",
    "        new_img_slice = new_img_slice[np.newaxis,:,:]\n",
    "        new_img_slice = normalise_img(new_img_slice)\n",
    "        # Add new image onto img slice array.\n",
    "        img_slice[i,:,:,:] = new_img_slice\n",
    "\n",
    "    # keep the relevant image labels\n",
    "    img_slice = img_slice[keep_idx,:,:,:]\n",
    "    img_labels = img_labels[keep_idx].reset_index(drop=True)\n",
    "\n",
    "    return img_labels, img_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "daf4a630-e511-4fd2-b1f3-e2bba0177157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_img(img):\n",
    "    \"\"\"\n",
    "    Applies min-max normalisation on an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : numpy array\n",
    "        An image array of integers or floats\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img : numpy array\n",
    "        A min-max normalised image\n",
    "    \"\"\"\n",
    "    # Normalise the input image.\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed204434-3edd-456e-955b-e8271bde8fc6",
   "metadata": {},
   "source": [
    "#### Get Image Slices and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02b7d436-53f7-49dd-8b5e-8a214de7fc3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_img_labels, test_img_slice = get_img_slices_and_labels(test_df)\n",
    "val_img_labels, val_img_slice = get_img_slices_and_labels(val_df)\n",
    "train_img_labels, train_img_slice = get_img_slices_and_labels(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39a338c6-bab6-4fca-b564-bc70b20eea63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if images are normalised\n",
    "test_img_slice.max()\n",
    "test_img_slice.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eef203e-d5e3-4329-80ad-e775ca6b8ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaN values in the images\n",
    "sum(sum(sum(np.isnan(test_img_slice))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c753ce53-c519-41ad-8fbf-f44981f6f7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9173, 120, 120, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d20071-a799-47eb-bc10-a081ca5e110b",
   "metadata": {},
   "source": [
    "#### Generate True Positives and Non-Detections For Train, Test and Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30ac050f-b253-4692-8056-fc37fb0dfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slice(imsize_x = 120, imsize_y = 120, bmaj = 5, bmin = 6, bpa = 0*u.deg, inj_flux = 10, noise_level=2.5):\n",
    "    \"\"\"\n",
    "    Generate an image comprised of a source convolved with\n",
    "    a gaussian kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imsize_x : int, default 120\n",
    "        Number of rows in the image array\n",
    "\n",
    "    imsize_y : int, default 120\n",
    "        Number of columns in the image array\n",
    "\n",
    "    b_maj : int, default 5\n",
    "        Used to calculate standard deviation of gaussian kernel in x\n",
    "\n",
    "    b_min : int, default 6\n",
    "        Used to calculate standard deviation of gaussian kernel in y\n",
    "\n",
    "    bpa : float, default 0\n",
    "        Angle of the gaussian kernel's orientation\n",
    "\n",
    "    inj_flux : int, default 10\n",
    "        Used to calculate flux of the source\n",
    "\n",
    "    noise_level : float, default 2.5\n",
    "        The noise level of the image calculated using a normal distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    injected_data : numpy array\n",
    "        An image array composed of gaussian noise combined with a central source\n",
    "        of flux\n",
    "    \"\"\"\n",
    "    \n",
    "    inj_position = (int(imsize_x/2), int(imsize_y/2))\n",
    "    \n",
    "    noise_data = np.random.normal(loc=0, scale=noise_level, size=(imsize_x, imsize_y))\n",
    "    \n",
    "    psf_kernel = Gaussian2DKernel(bmaj/2, bmin/2, bpa)\n",
    "    psf_kernel.normalize('peak')\n",
    "\n",
    "    gaussian_noise_data = convolve(noise_data, psf_kernel)\n",
    "    gaussian_noise_data *= noise_data.std()/gaussian_noise_data.std()\n",
    "    \n",
    "    source_inj = Gaussian2DKernel(bmaj/2, bmin/2, bpa, x_size=imsize_x, y_size=imsize_y)\n",
    "    source_inj.normalize('peak')\n",
    "    source_inj *= inj_flux\n",
    "    injected_data = gaussian_noise_data+source_inj.array\n",
    "    \n",
    "    return injected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "199efc6a-12ce-43a4-a657-bc7c22989f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_num_of_imgs(img_labels, label, inj_flux_bds, b_thresh = 1, noise_level = 2.5, imsize_x = 120, imsize_y = 120):\n",
    "    \"\"\"\n",
    "    Generate a number of images with the corresponding\n",
    "    label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_labels : pandas series\n",
    "        The image labels\n",
    "\n",
    "    label : int\n",
    "        0 for non detections and 1 for true positives\n",
    "\n",
    "    inj_flux_bds : tuple\n",
    "        The bounds for inj_flux which is used to generate the flux of the sources\n",
    "\n",
    "    b_thresh : int, default 1\n",
    "        Threshold value to determine the standard deviation of the gaussian\n",
    "        kernel\n",
    "\n",
    "    noise_level : float, default 2.5\n",
    "        The noise level of the image calculated using a normal distribution\n",
    "\n",
    "    imsize_x : int, default 120\n",
    "        Number of rows in the image array\n",
    "\n",
    "    imsize_y : int, default 120\n",
    "        Number of columns in the image array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_arr : numpy array\n",
    "        An array of generated images with the relevant label (either true positive\n",
    "        or non detection)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_gen = len(img_labels[img_labels == 2]) - len(img_labels[img_labels == label])\n",
    "    inj_position = (int(imsize_x/2), int(imsize_y/2))\n",
    "    img_arr = np.empty(shape=(n_gen,120,120,3))\n",
    "    for n in range(n_gen):\n",
    "    \n",
    "        inj_flux = random.uniform(inj_flux_bds[0],inj_flux_bds[1]) # change for nnon detections perhaps 0 to 12\n",
    "        bvals = random.sample(range(5-b_thresh, 5+b_thresh), 2)\n",
    "        bmin = min(bvals)\n",
    "        bmaj = max(bvals)\n",
    "        bpa = random.uniform(0,360)*u.deg\n",
    "        \n",
    "        img = generate_slice(imsize_x = imsize_x, imsize_y = imsize_y, bmaj = bmaj, bmin = bmin, bpa = bpa, inj_flux = inj_flux, noise_level=noise_level)\n",
    "        img = np.repeat(img[...,np.newaxis], 3, -1)\n",
    "        img = img[np.newaxis,:,:]\n",
    "        img = normalise_img(img)\n",
    "        img_arr[n,:,:,:] = img\n",
    "\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd780891-dafc-4fb9-986d-f8c3104ecdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rearrange_imgs_and_labels(img_slice, img_labels, random_state = 43):\n",
    "    \"\"\"\n",
    "    Generate the simulated non detection and true positive images.\n",
    "    Then combine them with the existing data obtained from the fits\n",
    "    cubes and light curve data before finally shuffling the images\n",
    "    and their corresponding labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_slice : numpy array\n",
    "        The images obtained from the light curve dataframe\n",
    "        and fits files.\n",
    "\n",
    "    img_labels : series\n",
    "        The labels for the images obtained from the light curve dataframes\n",
    "        and fits files.\n",
    "\n",
    "    random_state : int, default 43\n",
    "        Set a random state for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_slice : float\n",
    "        The image slices combined with the simulated images\n",
    "        and shuffled\n",
    "\n",
    "    img_labels : float\n",
    "        The image labels combined with the simulated images' \n",
    "        labels and shuffled\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the images\n",
    "    tp_img_arr = generate_num_of_imgs(img_labels, 1, (15,100))\n",
    "    nd_img_arr = generate_num_of_imgs(img_labels, 0, (0,12))\n",
    "    \n",
    "    # Can just concatenate the corresponding labels\n",
    "    nd_labels = pd.Series(np.zeros(nd_img_arr.shape[0]))\n",
    "    tp_labels = pd.Series(np.ones(tp_img_arr.shape[0]))\n",
    "    img_labels = pd.concat([img_labels,nd_labels,tp_labels], ignore_index=True)\n",
    "    # concate true positive images, nd images and the relevant data frame together.\n",
    "    img_slice = np.concatenate([img_slice, nd_img_arr, tp_img_arr], axis=0)\n",
    "    # Randomise label indices and then randomise images usng these indices\n",
    "    img_labels = img_labels.sample(frac=1, random_state = random_state, axis=0)\n",
    "    img_labels = img_labels.astype(int)\n",
    "    \n",
    "    rearr_idx = img_labels.index\n",
    "    img_slice = img_slice[rearr_idx]\n",
    "    \n",
    "    return img_slice, img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8613ccb9-27ef-4da3-8ddf-edb26f8d4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_img_slice, final_test_img_labels = gen_rearrange_imgs_and_labels(test_img_slice, test_img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32ad06bf-490d-4a37-9c59-ba813295cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_val_img_slice, final_val_img_labels = gen_rearrange_imgs_and_labels(val_img_slice, val_img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0583b403-ca32-4102-b3f0-0cef57429743",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_img_slice, final_train_img_labels = gen_rearrange_imgs_and_labels(train_img_slice, train_img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43017ade-1dc9-4a3e-8eb2-77c446aead58",
   "metadata": {},
   "source": [
    "#### Save The Image Slices With Simulated Images and Their Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60cb746b-3a01-4f31-944b-071757d06346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_slice(img_slice_data, img_labels_data, data_class, data_fol = \"CNN_Data\"):\n",
    "    \"\"\"\n",
    "    Save the images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_slice_data : numpy array\n",
    "        The images\n",
    "\n",
    "    img_labels : pandas series\n",
    "        The labels for the images\n",
    "\n",
    "    data_class : str\n",
    "        Whether the image belongs to \"Train\", \"Validation\" or \"Test\" folders\n",
    "\n",
    "    data_fol : str, default \"CNN_Data\"\n",
    "        The folder the images will be saved in\n",
    "    \"\"\"\n",
    "    \n",
    "    f_path = f\"{data_fol}/{data_class}/{data_class}\"\n",
    "    np.save(f_path+\"_With_Sim_Imgs\", img_slice_data)\n",
    "    np.save(f_path+\"_With_Sim_Labels\", img_labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b920b13-eb12-45ee-bdc5-7ed3aa275242",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img_slice(final_test_img_slice, final_test_img_labels, \"Test\", data_fol = \"CNN_Data\")\n",
    "save_img_slice(final_train_img_slice, final_train_img_labels, \"Train\", data_fol = \"CNN_Data\")\n",
    "save_img_slice(final_val_img_slice, final_val_img_labels, \"Validation\", data_fol = \"CNN_Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
